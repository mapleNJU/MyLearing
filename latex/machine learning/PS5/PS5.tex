% 请确保文件编码为utf-8，使用XeLaTex进行编译，或者通过overleaf进行编译

\documentclass[answers]{exam}  % 使用此行带有作答模块
% \documentclass{exam} % 使用此行只显示题目

\usepackage{xeCJK}
\usepackage{zhnumber}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{floatrow}
\usepackage{blindtext}
\usepackage{subcaption}
\pagestyle{headandfoot}
\firstpageheadrule
\firstpageheader{南京大学}{机器学习导论}{习题五}
\runningheader{南京大学}
{机器学习导论}
{习题五}
\runningheadrule
\firstpagefooter{}{第\thepage\ 页（共\numpages 页）}{}
\runningfooter{}{第\thepage\ 页（共\numpages 页）}{}


\setlength\linefillheight{.8in}

\renewcommand{\solutiontitle}{\noindent\textbf{解：}\par\noindent}

\renewcommand{\thequestion}{\zhnum{question}}
\renewcommand{\questionlabel}{\thequestion .}
\renewcommand{\thepartno}{\arabic{partno}}
\renewcommand{\partlabel}{\thepartno .}

\lstset{language=Matlab}%这条命令可以让LaTeX排版时将Matlab关键字突出显示
\lstset{
	breaklines,%这条命令可以让LaTeX自动将长的代码行换行排版
	basicstyle=\footnotesize\ttfamily, % Standardschrift
	backgroundcolor=\color[rgb]{0.95,0.95,0.95},
	keywordstyle=\color{blue},
	commentstyle=\color{cyan},
	tabsize=4,numbers=left,
	numberstyle=\tiny,
	frame=single,
	%numbers=left, % Ort der Zeilennummern
	numberstyle=\tiny, % Stil der Zeilennummern
	%stepnumber=2, % Abstand zwischen den Zeilennummern
	numbersep=5pt, % Abstand der Nummern zum Text
	tabsize=2, % Groesse von Tabs
	extendedchars=false, %
	breaklines=true, % Zeilen werden Umgebrochen
	keywordstyle=\color{red},%这一条命令可以解决代码跨页时, 章节标题, 页眉等汉字不显示的问题
	stringstyle=\color{white}\ttfamily, % Farbe der String
	showspaces=false, % Leerzeichen anzeigen ?
	showtabs=false, % Tabs anzeigen ?
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	%backgroundcolor=\color{lightgray},
	showstringspaces=false % Leerzeichen in Strings anzeigen ?
}
\renewcommand{\lstlistingname}{CODE}
\lstloadlanguages{% Check Dokumentation for further languages ...
	%[Visual]Basic
	%Pascal
	%C
	Python
	%XML
	%HTML
	%Java
}
\input{notations}

\begin{document}
% \Large
\noindent
% 姓名学号
姓名：麻超 \\
学号：201300066 \\
\begin{questions}
	\question [20] \textbf{贝叶斯决策论} \\
	教材7.1节介绍了贝叶斯决策论, 它是一种解决统计决策问题的通用准则. 考虑一个带有“拒绝”选项的$N$分类问题, 给定一个样例, 分类器可以选择预测这个样例的标记, 也可以选择拒绝判断并将样例交给人类专家处理. 设类别标记的集合为$\mathcal{Y}=\{c_1,c_2,\ldots,c_N\}$, $\lambda_{ij}$是将一个真实标记为$c_i$的样例误分类为$c_j$所产生的损失, 而人类专家处理一个样例需要额外$\lambda_{h}$费用. 假设后验概率$P(c\mid\vx)$已知, 且$\lambda_{ij}\geq 0$, $\lambda_{h}\geq 0$. 请思考下列问题:
	\begin{enumerate}
		\item 基于期望风险最小化原则, 写出此时贝叶斯最优分类器$h^\star(\vx)$的表达式;
		\item 人类专家的判断成本$\lambda_{h}$取何值时, 分类器$h^\star$将一直拒绝分类? 当$\lambda_{h}$取何值时, 分类器$h^\star$不会拒绝分类任何样例?
		\item 考虑一个具体的二分类问题, 其损失矩阵为
		      \begin{equation}
			      \mat{\Lambda}=\left(\begin{array}{cc}
					      \lambda_{11} & \lambda_{12} \\
					      \lambda_{21} & \lambda_{22}\end{array}\right)=\left(\begin{array}{cc}
					      0 & 1 \\
					      1 & 0\end{array}\right)\;,
			      \label{ch7_eq:cost_matrix}
		      \end{equation}
		      且人类专家处理一个样例的代价为$\lambda_{h}=0.3$. 对于一个样例$\vx$, 设$p_1=P(c_1\mid\vx)$, 证明存在$\theta_1,\theta_2\in[0,1]$, 使得贝叶斯最优决策恰好为: 当$p_1<\theta_1$时, 预测为第二类, 当$\theta_1\leq p_1\leq \theta_2$时, 拒绝预测, 当$\theta_2<p_1$时, 预测为第一类.
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item 可以定义判定准则$h:\mathcal{X}\rightarrow \mathcal{Y}$的条件风险为:
			      \begin{align*}
				      R(h(\vx)|\vx)=\min \ (\min\limits_{c_i\in \mathcal{Y}}\ R(c_i|x),\lambda_h)
			      \end{align*}

			      故应当寻找一个h以最小化期望风险:
			      \begin{align*}
				      R(h)=\mathbb{E}_x[R(h(\vx)|\vx)]
			      \end{align*}

			      所以最小化条件风险即可得到判定准则:
			      \begin{align*}
				      h^*(\vx)=\argmin\limits_{c\in \mathcal{Y}}\ R(c|x)
			      \end{align*}
			\item 当满足条件$\lambda_{h}\leq\min\limits_{\vx\in\mathcal{X}}(\min\limits_{c\in\mathcal{Y}}P(c|x))$时,分类器会一直拒绝分类.

			      当满足条件$\lambda_{h}>\max\limits_{\vx\in\mathcal{X}}(\min\limits_{c\in\mathcal{Y}}P(c|x))$时,分类器不会拒绝分类.
			\item 首先计算各个类别的条件风险值:
			      \begin{align*}
				      R(c_1|\vx)=P(c_2|\vx)=1-p_1,R(c_2|\vx)=P(c_1|\vx)=p_1 \\
				      \lambda_h=0.3
			      \end{align*}

			      当$p_1<\theta_1$时,预测为第二类,则有:
			      \begin{align*}
				       & p_1\leq 1-p_1 \\
				       & p_1\leq 0.3
			      \end{align*}

			      得到$\theta_1\leq 0.3$.

			      当$\theta_1<p_1<\theta_2$时,有:
			      \begin{align*}
				       & 1-p_1\geq 0.3 \\
				       & p_1\geq 0.3
			      \end{align*}

			      得到$\theta_1\geq 0.3,\theta_2\leq 0.7$

			      当$p_1>\theta_2$时,有:
			      \begin{align*}
				       & 1-p_1\leq p_1 \\
				       & 1-p_1\leq 0.3
			      \end{align*}

			      得到$\theta_2\geq 0.7$

			      所以存在$\theta_1=0.3,\theta_2=0.7$满足题目要求.
		\end{enumerate}
	\end{solution}

	\question [20] \textbf{极大似然估计} \\
	教材7.2节介绍了极大似然估计方法用于确定概率模型的参数. 其基本思想为: 概率模型的参数应当使得当前观测到的样本是最有可能被观测到的, 即当前数据的似然最大. 本题通过抛硬币的例子理解极大似然估计的核心思想.
	\begin{enumerate}
		\item 现有一枚硬币, 抛掷这枚硬币后它可能正面向上也可能反面向上. 我们已经独立重复地抛掷了这枚硬币$99$次, 均为正面向上. 现在, 请使用极大似然估计来求解第$100$次抛掷这枚硬币时其正面向上的概率;
		\item 仍然考虑上一问的问题. 但现在, 有一位抛硬币的专家仔细观察了这枚硬币, 发现该硬币质地十分均匀, 并猜测这枚硬币“肯定有$50\%$的概率正面向上”. 如果同时考虑已经观测到的数据和专家的见解, 第$100$次抛掷这枚硬币时, 其正面向上的概率为多少?

		\item 若同时考虑专家先验和实验数据来对硬币正面朝上的概率做估计. 设这枚硬币正面朝上的概率为$\theta$, 某抛硬币专家主观认为$\theta\sim\mathcal{N}(\frac{1}{2}, \frac{1}{900})$, 即$\theta$服从均值为$\frac{1}{2}$, 方差为$\frac{1}{900}$的高斯分布. 另一方面, 我们独立重复地抛掷了这枚硬币$400$次, 记第$i$次的结果为$x_i$, 若$x_i=1$则表示硬币正面朝上, 若$x_i=0$则表示硬币反面朝上. 经统计, 其中有$100$次正面向上, 有$300$次反面向上. 现在, 基于专家先验和观测到的数据$\vx=\{x_1, x_2, \ldots, x_{400}\}$, 对参数$\theta$分别做极大似然估计和最大后验估计;

		\item 如何理解上一小问中极大似然估计的结果和最大后验估计的结果？
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item 设样本满足参数为$\theta$的0-1分布,则其似然为:
			      \begin{align*}
				      L(D|\theta)=\prod_{x\in D}P(x|\theta)=\theta^{99}.
			      \end{align*}

			      其对数似然为:
			      \begin{align*}
				      \ln L(D|\theta)=99\ln \theta.
			      \end{align*}

			      该对数似然函数为关于$\theta$的增函数,故$\theta$的极大似然估计为$\hat{\theta}=1$\dots

			      所以第100次掷硬币正面朝上概率的极大似然估计为1.
			\item 无法使用最大后验估计.根据其说法,假设前99次都正面朝上为事件A,事件B为掷一次硬币结果为正面朝上,则有$P(B)=0.5$.可得:
			      \begin{align*}
				      P(B|A)=\frac{P(B)P(A|B)}{P(A)}=0.5.
			      \end{align*}

			      所以第100次正面朝上概率为0.5
			\item 极大似然估计：
			      \begin{align*}
				      \hat{D|\theta}=\argmax\limits_{\theta}\theta^{100}(1-\theta)^{300} \\
				      L(D|\theta)=\theta^{100}(1-\theta)^{300}                           \\
				      LL(D|\theta)=100\ln\theta+300\ln(1-\theta)                         \\
			      \end{align*}

			      对对数似然函数中的$\theta$求偏导可得:
			      \begin{align*}
				      \frac{\partial LL(\theta)}{\partial \theta}=0
			      \end{align*}

			      可得$\hat{\theta}=\frac{1}{4}$

			      最大后验估计：

			      可以得到后验函数与指数后验函数如下:
			      \begin{align*}
				      Pos(\theta|D)     & =\theta^{100}(1-\theta)^{300}\frac{1}{\frac{1}{30}\sqrt{2\pi}}e^{-\frac{(\theta-\frac{1}{2})^2}{2\cdot\frac{1}{900}}} \\
				      \ln Pos(\theta|D) & =100(\ln \theta+300\ln (1-\theta))-450(\theta-\frac{1}{2} )^2+c                                                       \\
			      \end{align*}

			      对对数后验函数的参数$\theta$求偏导:
			      \begin{align*}
				      2(\frac{1}{\theta} -\frac{3}{1-\theta} )-18(\theta-\frac{1}{2} )=0,0\leq \theta\leq 1.
			      \end{align*}

			      可得$\theta$的最大后验估计为$\hat{\theta}=\frac{1}{3}$
			\item 极大似然估计完全依靠已有样本来估计概率，只会受到样本集内容的影响，通过调节未知参数使已有数据出现概率最大.

			      而最大后验估计考虑了先验概率的影响，因此与极大似然估计结果不同。在极大似然估计的基础上，对参数假设其服从一个先验分布,根据得到的数据计算参数的后验分布.不仅关注已有的样本数据,还会通过调节未知参数使得已有数据出现概率最大.
		\end{enumerate}
	\end{solution}


	\question [20] \textbf{朴素贝叶斯分类器} \\
	朴素贝叶斯算法有很多实际应用, 本题以sklearn中的Iris数据集为例, 探讨实践中朴素贝叶斯算法的技术细节. 可以通过sklearn中的内置函数直接获取Iris数据集, 代码如下:
	\begin{lstlisting}[language=Python]
def load_data():
    # 以feature, label的形式返回数据集
    feature, label = datasets.load_iris(return_X_y=True)
    print(feature.shape) # (150, 4)
    print(label.shape) # (150,)
    return feature, label
\end{lstlisting}
	上述代码返回Iris数据集的特征和标记, 其中feature变量是形状为$(150, 4)$的numpy数组, 包含了$150$个样本的$4$维特征, 而label变量是形状为$(150)$的numpy数组, 包含了$150$个样本的类别标记. Iris数据集中一共包含$3$类样本, 所以类别标记的取值集合为$\{0,1,2\}$. Iris数据集是类别平衡的, 每类均包含$50$个样本. 我们进一步将完整的数据集划分为训练集和测试集, 其中训练集样本量占总样本量的$80\%$, 即$120$个样本, 剩余$30$个样本作为测试样本.
	\begin{lstlisting}[language=Python]
feature_train, feature_test, label_train, label_test = \
    train_test_split(feature, label, test_size=0.2, random_state=0)
\end{lstlisting}
	朴素贝叶斯分类器会将一个样例的标记预测为类别后验概率最大的那一类对应的标记, 即:
	\begin{equation}
		\hat{y}=\argmax_{y\in\{0,1,2\}}P(y)\prod_{i=1}^{d}P(x_i\mid y)\;.
	\end{equation}
	因此, 为了构建一个朴素贝叶斯分类器, 我们需要在{\em 训练集}上获取所有类别的先验概率$P(y)$以及所有类别所有属性上的类条件概率$P(x_i\mid y)$.

	\begin{enumerate}
		\item 请检查训练集上的类别分布情况, 并基于多项分布假设对$P(y)$做极大似然估计;
		\item 在Iris数据集中, 每个样例$\vx$都包含$4$维实数特征, 分别记作$x_1$, $x_2$, $x_3$和$x_4$. 为了计算类条件概率$P(x_i\mid y)$, 首先需要对$P(x_i\mid y)$的概率形式做出假设. 在本小问中, 我们假设每一维特征在给定类别标记时是独立的（朴素贝叶斯的基本假设）, 并假设它们服从高斯分布. 试基于sklearn中的GaussianNB类构建分类器, 并在测试集上测试性能;
		\item 在GaussianNB类中手动指定类别先验为三个类上的均匀分布, 再次测试模型性能;
		\item 在朴素贝叶斯模型中, 对类条件概率的形式做出正确的假设也很重要. 请检查每个类别下特征的数值分布, 并讨论该如何选定类条件概率的形式.
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item 对训练集中的样本进行分析,可以得到其中第0类样本数量为39,第一类为37,第二类为44.故可以对该样本进行分析.令$p_0=P(y=0),p_1=P(y=1),p_2=P(y=2)$,为了让当前样本出现的概率最大,故可以得到如下优化问题:
			      \begin{align*}
				      \max\limits_{p_0,p_1,p_2}\  & \frac{120!}{n_0!n_1!n_2!} p_0^{n_0}p_1^{n_1}p_2^{n_2} \\
				      s.t.\                       & p_0+p_1+p_2=1                                         \\
				                                  & 0\leq p_0,p_1,p_2\leq 1.
			      \end{align*}

			      故其对数似然可以写作如下形式:
			      \begin{align*}
				      \ln LL=n_0\ln p_0+n_1\ln p_1+n_2\ln(1-p_0-p_1)
			      \end{align*}

			      对$p_0,p_1$求偏导:
			      \begin{align*}
				      \frac{\partial \ln LL}{\partial p_0}=\frac{n_0}{p_0} -\frac{n_2}{1-p_0-p_2} =0 \\
				      \frac{\partial \ln LL}{\partial p_1}=\frac{n_1}{p_1} -\frac{n_2}{1-p_0-p_2} =0
			      \end{align*}

			      根据以上方程可解得:
			      \begin{align*}
				      \begin{cases}
					      p_0 & =\frac{n_0}{n} =\frac{39}{120} \\
					      p_1 & =\frac{n_1}{n} =\frac{37}{120} \\
					      p_2 & =\frac{n_2}{n} =\frac{44}{120}
				      \end{cases}
			      \end{align*}


		\end{enumerate}
	\end{solution}
	\question [20] \textbf{Boosting} \\
	Boosting算法有序地训练一批弱学习器进行集成得到一个强学习器, 核心思想是使用当前学习器``提升"已训练弱学习器的能力. 教材8.2节介绍的AdaBoost是一种典型的Boosting算法, 通过调整数据分布使新学习器重点关注之前学习器分类错误的样本. 教材介绍的AdaBoost关注的是二分类问题, 即样本$\vx$对应的标记$y(\vx)\in \{-1, +1\}$. 记第$t$个基学习器及其权重为$h_t$和$\alpha_t$, 采用$T$个基学习器加权得到的集成学习器为$H(\vx)=\sum_{t=1}^T \alpha_t h_t(\vx)$. AdaBoost最小化指数损失: $\ell_{\text{exp}}=\mathbb{E}_{\vx \sim \mathcal{D}}\left[ e^{-y(\vx)H(\vx)} \right]$.

	% 不同的``提升"方法对应不同的Boosting算法. 

	% 则第$t$时刻和第$t-1$时刻得到的集成模型的关系为: $H_t(\vx)=H_{t-1}(\vx)+\alpha_t h_t(\vx)$

	\begin{enumerate}
		\item  在AdaBoost训练过程中, 记前$t$个弱学习器的集成为$H_t(\vx) = \sum_{i=1}^t \alpha_i h_i(\vx)$, 该阶段优化目标为:
		      \begin{equation}
			      \ell_{\text{exp},t} = \mathbb{E}_{\vx \sim \mathcal{D}}\left[ e^{-y(\vx)H_t(\vx)} \right]. \label{ch8_eq:boost-exp-loss}
		      \end{equation}
		      如果记训练数据集的初始分布为$\mathcal{D}_0=\mathcal{D}$, 那么第一个弱学习器的训练依赖于数据分布$\mathcal{D}_0$. AdaBoost根据第一个弱学习器的训练结果将训练集数据分布调整为$\mathcal{D}_1$, 然后基于$\mathcal{D}_1$训练第二个弱学习器. 依次类推, 训练完前$t-1$个学习器之后的数据分布变为$\mathcal{D}_{t-1}$. 根据以上描述并结合``加性模型"(Additive Model), 请推导AdaBoost调整数据分布的具体过程, 即$\mathcal{D}_t$与$\mathcal{D}_{t-1}$的关系;
		\item AdaBoost算法可以拓展到$N$分类问题. 现有一种设计方法, 将样本标记编码为$N$维向量$\vy$, 其中目标类别对应位置的值为$1$, 其余类别对应位置的值为$-\frac{1}{N-1}$. 这种编码的一种性质是$\sum_{n=1}^N \vy_n = 0$, 即所有类别对应位置的值的和为零. 同样地, 学习器的输出为一个$N$维向量, 且约束其输出结果的和为零, 即: $\sum_{n=1}^N [h_t(\vx)]_n = 0$. $[h_t(\vx)]_{n}$表示基分类器输出的$N$维向量的第$n$个值. 在这种设计下, 多分类情况下的指数损失为:
		      \begin{equation}
			      \ell_{\text{multi-exp}} = \mathbb{E}_{\vx \sim \mathcal{D}}\left[  e^{-\frac{1}{N}\sum_{n=1}^N \vy_n [H(\vx)]_n} \right] = \mathbb{E}_{\vx \sim \mathcal{D}}\left[  e^{- \frac{1}{N} \vy^\top H(\vx)} \right]. \label{ch8_eq:boost-multi-exp-loss}
		      \end{equation}
		      请分析为何如此设计;
		\item 教材8.2节已经证明AdaBoost在指数损失下得到的决策函数$\text{sign}(H(\vx))$可以达到贝叶斯最优误差. 仿照教材中的证明, 请从贝叶斯最优误差的角度验证式\eqref{ch8_eq:boost-multi-exp-loss}的合理性.
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item 理想的基学习器$h_t$可以纠正之前的加权分类器$H_{t-1}$的错误,可以得到:
			      $$
				      h_t(x)=\underset{h}{\mathrm{arg\max}}\ l_{exp}(H_{t-1}+h_t|\mathcal{D})\\=arg \max\limits_{h}\mathbb{E}_{\vx\sim\mathcal{D}}[\frac{e^{-f(\vx)H_{t-1}(\vx)}}{\mathbb{E}_{\vx\sim\mathcal{D}}[e^{-f(\vx)H_{t-1}(\vx)}]}f(\vx)h(\vx) ]
			      $$

			      生成一个新的数据分布,并使$h_t(\vx)$的生成基于这个分布,也就是:
			      $$
				      h_t(\vx)=arg \max\limits_{h}\mathbb{E}_{\vx\sim\mathcal{D}_t}[f(\vx)h(\vx)]
			      $$

			      令$\mathcal{D}_t$表示一个分布,则
			      $$
				      \mathcal{D}_t(\vx)=\frac{\mathcal{D}(x)e^{-f(\vx)H_{t-1}(\vx)}}{\mathbb{E}_{\vx\sim\mathcal{D}}[e^{-f(\vx)H_{t-1}(\vx)}]}
			      $$

			      则有:
			      $$
				      \mathcal{D}_t(\vx)=\mathcal{D}_{t-1}(\vx)e^{-f(\vx)\alpha_{t-1}h_{t-1}(\vx)}\frac{\mathbb{E}_{\vx\sim\mathcal{D}}[e^{-f(\vx)H_{t-2}(\vx)}]}{\mathbb{E}_{\vx\sim\mathcal{D}}[e^{-f(\vx)H_{t-1}(\vx)}]}
			      $$
			\item 该损失函数有特定的惩罚机制,它会惩罚那些和答案偏差较大的结果,对结果与答案之间相似程度进行判定,若相似程度较大则会有较小惩罚值,若不相似则有较大惩罚值,同时,该最小化损失函数婚获得一个等晓得贝叶斯分类器,以达到理论最优解.
			\item 即对于一个理想的基学习器对其多元指数损失函数最小化,也就是:
			      \begin{align*}
				      \underset{H(\vx)}{\mathrm{arg\min}}\  & mathbb{E}_{\vx\sim\mathcal{D}}[e^{-\frac{1}{N} \sum_{n=1}^{N}\vy_n[H(\vx)]_n}] \\
				      s.t.\                                 & \sum_{n=1}^{N}[H(\vx)]_n=0
			      \end{align*}
			      原问题的Lagrange函数为:
			      \begin{align*}
				      exp(-\frac{[H(\vx)_1]}{N-1} )Pr(\vy_1|\vx)+...+exp(-\frac{[H(\vx)_n]}{N-1} )Pr(\vy_n|\vx)-\lambda(\sum_{n=1}^{N}[H(\vx)]_n)
			      \end{align*}
			      对$[H(\vx)]_n$和$\lambda$分别求导:
			      \begin{align*}
				      -\frac{1}{N-1} exp(-\frac{[H(\vx)_1]}{N-1} )Pr(\vy_1|\vx)-\lambda=0 \\
				      ...                                                                 \\
				      -\frac{1}{N-1} exp(-\frac{[H(\vx)_N]}{N-1} )Pr(\vy_N|\vx)-\lambda=0 \\
				      \sum_{n=1}^{N}[H(\vx)]_n=0
			      \end{align*}
			      解得:
			      \begin{align*}
				      [H(\vx)]_k=(N-1)\log \ Pr(\vy_k|\vx)-\frac{N-1}{N} \sum_{k^{'}=1}^{N}\log \ Pr(\vy_{k^{'}}|\vx)
			      \end{align*}
			      所以可以得到:
			      \begin{align*}
				      \underset{k}{\mathrm{arg \max}}\ [H(\vx)]_k=\underset{k}{\mathrm{arg \max}}\ Pr(\vy_k|\vx)
			      \end{align*}
			      故可以得知其最大化后验概率,故就是一个贝叶斯分类器.
		\end{enumerate}
	\end{solution}

	\question [20] \textbf{Bagging} \\
	考虑一个回归学习任务$f:\mathbb{R}^d \rightarrow \mathbb{R}$. 假设已经学得$T$个学习器$\{h_1(\vx), h_2(\vx), \dots, h_T(\vx)\}$. 将学习器的预测值视为真实值项加上误差项:
	\begin{equation}
		h_t(\vx)=y(\vx)+\epsilon_t(\vx).
	\end{equation}

	每个学习器的期望平方误差为$\mathbb{E}_{\vx}[\epsilon_t(\vx)^2]$. 所有学习器的期望平方误差的平均值为:
	\begin{equation}
		E_{av}=\frac{1}{T}\sum_{t=1}^T \mathbb{E}_{\vx}[\epsilon_t(\vx)^2].
	\end{equation}

	$T$个学习器得到的Bagging模型为:
	\begin{equation}
		H_{bag}(\vx)=\frac{1}{T}\sum_{t=1}^T h_t(\vx).
	\end{equation}

	Bagging模型的误差为:
	\begin{equation}
		\epsilon_{bag}(\vx)=H_{bag}(\vx)-y(\vx)=\frac{1}{T}\sum_{t=1}^T \epsilon_t(\vx),
	\end{equation}

	其期望平均误差为:
	\begin{equation}
		E_{bag}=\mathbb{E}_{\vx}[\epsilon_{bag}(\vx)^2].
	\end{equation}

	\begin{enumerate}
		\item 假设$\forall t\neq l$, $\mathbb{E}_{\vx}[\epsilon_t(\vx)]=0$, $ \mathbb{E}_{\vx}[\epsilon_t(\vx)\epsilon_l(\vx)]=0$. 证明:
		      \begin{equation}
			      E_{bag}=E_{av}.
		      \end{equation}

		\item 请证明无需对$\epsilon_t(\vx)$做任何假设, $E_{bag}\leq E_{av}$始终成立.
	\end{enumerate}

	\begin{solution}
		\begin{enumerate}
			\item \begin{align*}
				      E_{bag} & =\mathbb{E}_{\vx}[\epsilon_{bag}(\vx)^2]
				      \\&=\mathbb{E}_{\vx}[(\frac{1}{T}\sum_{t=1}^{T}\epsilon_t(\vx))^2]\\
				              & =\frac{1}{T^2}\mathbb{E}_{\vx}[\sum_{t=1}^{T}\epsilon_t(\vx)^2+2\sum_{1\leq i\textless j\leq T}\epsilon_i(\vx)\epsilon_j(\vx)]                     \\
				              & =\frac{1}{T^2}(\mathbb{E}_{\vx}[\sum_{t=1}^{T}\epsilon_t(\vx)^2]+2\sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[\epsilon_i(\vx)\epsilon_j(\vx)]) \\
				              & =\frac{1}{T^2}(\sum_{t=1}^T \mathbb{E}_{\vx}[\epsilon_t(\vx)^2])                                                                                   \\
				              & =\frac{1}{T}E_{av}
			      \end{align*}
			\item \begin{align*}
				      \because   & E_{bag}=\frac{1}{T^2}(\mathbb{E}_{\vx}[\sum_{t=1}^{T}\epsilon_t(\vx)^2]+2\sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[\epsilon_i(\vx)\epsilon_j(\vx)]) \\
				      \text{and} & E_{av}=\frac{1}{T}\sum_{t=1}^T \mathbb{E}_{\vx}[\epsilon_t(\vx)^2]
			      \end{align*}

			      故欲证$E_{bag}\leq E_{av}$,即证
			      \begin{align*}
				       & \frac{1}{T^2}(\mathbb{E}_{\vx}[\sum_{t=1}^{T}\epsilon_t(\vx)^2]+2\sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[\epsilon_i(\vx)\epsilon_j(\vx)])\leq\frac{1}{T}\sum_{t=1}^T \mathbb{E}_{\vx}[\epsilon_t(\vx)^2] \\
				       & 2\sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[\epsilon_i(\vx)\epsilon_j(\vx)]\leq (T-1)\sum_{t=1}^T \mathbb{E}_{\vx}[\epsilon_t(\vx)^2]                                                                       \\
				       & 2\sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[\epsilon_i(\vx)\epsilon_j(\vx)]\leq \sum_{1\leq i\textless j\leq T}(\mathbb{E}_{\vx}[\epsilon_i(\vx)^2]+\mathbb{E}_{\vx}[\epsilon_j(\vx)^2])                    \\
				       & \sum_{1\leq i< j\leq T}(\mathbb{E}_{\vx}[\epsilon_i(\vx)^2]-2\mathbb{E}_{\vx}[\epsilon_i(\vx)\epsilon_j(\vx)]+\mathbb{E}_{\vx}[\epsilon_j(\vx)^2])\geq0                                                          \\
				       & \sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[\epsilon_i(\vx)^2-2\epsilon_i(\vx)\epsilon_j(\vx)+\epsilon_j(\vx)^2]\geq0                                                                                        \\
				       & \sum_{1\leq i\textless j\leq T}\mathbb{E}_{\vx}[(\epsilon_i(\vx)-\epsilon_j(\vx))^2]\geq0
			      \end{align*}

			      概率密度为非负,平方亦不小于0,故其正确,得证.
		\end{enumerate}
	\end{solution}

	\question [20] \textbf{$k$均值算法} \\
	\label{ch9_prob:kmeans}
	教材9.4.1节介绍了最经典的原型聚类算法---$k$均值算法 ($k$-means). 给定包含$m$个样本的数据集$D =\left\{\vx_{1}, \vx_{2}, \ldots, \vx_{m}\right\}$, 其中$k$是聚类簇的数目, $k$均值算法希望获得簇划分$\mathcal{C}=\left\{C_{1}, C_{2}, \cdots, C_{k}\right\}$
	使得教材式(9.24)最小化, 目标函数如下:
	\begin{equation}
		E=\sum_{i=i}^{k}\sum_{\vx \in C_{i}}\left\| \vx-\vu_{i} \right\|^{2}\;.
	\end{equation}
	其中$\vmu_{1}, \ldots, \vmu_{k}$为$k$个簇的中心. 目标函数$E$也被称作均方误差和~(Sum of Squared Error, SSE),
	这一过程可等价地写为最小化如下目标函数
	\begin{equation} \label{ch9_kmeans_obj}
		E\left(\mathbf{\Gamma}, \vmu_{1}, \ldots, \vmu_{k}\right)=\sum_{i=1}^{m} \sum_{j=1}^{k} {\Gamma}_{i j}\left\|\vx_{i}-\vmu_{j}\right\|^{2}\;.
	\end{equation}
	其中$\mathbf{\Gamma} \in \mathbb{R}^{m \times k}$为指示矩阵(indicator matrix)定义如
	下: 若$\vx_{i}$属于第$j$个簇, 即$\vx_i\in C_j$, 则${\Gamma}_{i j}=1$, 否则为0.
	$k$均值聚类算法流程如算法\ref{ch9_alg:kmeans}中所示~（即教材中图9.2所述算法）. 请回答以下问题:
	{\begin{algorithm}[ht]
		\caption{ $k$均值算法 }
		\label{ch9_alg:kmeans}
		\begin{algorithmic}[1]{
				\State 初始化所有簇中心 $\vmu_{1}, \ldots, \vmu_{k}$;
				\Repeat
				\State {\bf{Step 1:}} 确定 $\left\{\vx_{i}\right\}_{i=1}^{m}$所属的簇, 将它们分配到最近的簇中心所在的簇.
				\begin{align}{\Gamma}_{i j}=\left\{\begin{array}{ll}
						1, & \left\|\vx_{i}-\vmu_{j}\right\|^{2} \leq \left\|\vx_{i}-\vmu_{j^{\prime}}\right\|^{2}, \forall j^{\prime} \\
						0, & \text { otherwise }
					\end{array}\right.\end{align} \label{ch9_:step1}
				\State {\bf{Step 2:}} 对所有的簇 $j \in\{1, \cdots, k\}$, 重新计算簇内所有样本的均值, 得到新的簇中心$\vmu_j$  :
				\begin{align}\vmu_{j}=\frac{\sum_{i=1}^{m} {\Gamma}_{i j} \vx_{i}}{\sum_{i=1}^{m} {\Gamma}_{i j}}\end{align}

				\Until 目标函数 $J$ 不再变化.}
		\end{algorithmic}
	\end{algorithm}}
	\begin{enumerate}
		\item 请证明, 在算法\ref{ch9_alg:kmeans}中, Step 1和Step 2都会使目标函数$J$的值降低（或不增加）;
		\item 请证明, 算法\ref{ch9_alg:kmeans}会在有限步内停止;
		\item 请证明, 目标函数$E$的最小值是关于$k$的非增函数.
	\end{enumerate}


	\begin{solution}
		\begin{enumerate}
			\item Step 1:假设使用Step 1会使$J$的值增加,则说明至少存在一个点,使得它到当前簇中心的距离大于它到所有簇中心的距离中的最小值,即:
			      \begin{align*}
				      \exists i,j,j^{'}\ , \|\vx_i-\vmu_j\| > \|\vx_i-\vmu_{j^{'}}\|
			      \end{align*}
			      然而其违背了算法的划分依据,矛盾,故Step 1不会使$J$的值增加.

			      Step 2:对(15)式d对$\vmu_j$求偏导可得:
			      \begin{align*}
				      \sum_{i=1}^{m}\Gamma_{ij}(\vmu_j-\vx_i)=0 \\
				      \vmu_j=\frac{\sum_{i=1}^{m}\Gamma_{ij}\vx_i}{\sum_{i=1}^{m}\Gamma_{ij}}
			      \end{align*}
			      故新的簇中心就是簇内所有点距离和最小的点,故Step 2不会引起$J$值增加.
			\item 由题,共有m个样本以及k个簇,此时k-means算法一定会在有限的$k^m+1$次内停止.

			      理由如下:因为一共有k个簇,以及m个样本,故一共有$k^m$种可能的分类情况,由抽屉原理可知,假设执行$k^m+1$次时结果为n,由于其结果一定在前面出现过,所以其非递增,故而得到第$k^m+1$次的结果一定等于第$k^m$次,故算法此时一定停止了,即在有限步内停止.
			\item 假设当聚类簇数目为k时,目标函数$E$有最小值$E_0$.此时添加一个新的聚类簇中心,使其总数加1,重复执行算法1中的Step 1,Step 2,直到终止,会得到一个比$E_0$更小或者等于$E_0$的返回值,设为$E_1$.

			      而在聚类簇数目为$k+1$时目标函数理论最小值假设为$E_1^{'}$,有$E_1^{'}\leq E_1$,故$E_1^{'}\leq E_0$.所以可以证明目标函数$E$的最小值是关于k的非增函数.
		\end{enumerate}
	\end{solution}

\end{questions}




\end{document}